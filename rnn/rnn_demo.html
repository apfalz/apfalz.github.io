<html>
<head>
<title>Generating Audio Using RNNS</title>
 <meta charset="UTF-8">

<link href="https://fonts.googleapis.com/css?family=Arvo|Nunito+Sans|Pathway+Gothic+One" rel="stylesheet">

<link rel="stylesheet" type="text/css" href="rnn_demo.css">

</head>


<body onload="add_animations()">
  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-69191717-1', 'auto');
ga('send', 'pageview');

</script>
<!--=========================Title=========================-->
<div class="containing_div" id="title_cont">
    <h1 class="centered_text title">Generating Audio Using Recurrent Neural Networks</h1>
    <h2 class="centered_text title">a PhD Dissertation by Andrew Pfalz</h2>
    <p>This page is a brief overview of my dissertation work. I will explain:</p>
    <ul>
        <li>how to <span class="train_word">train</span> recurrent neural networks to predict audio</li>
        <li>how these models can be <span class="sampled_output_word">sampled</span> from to create new sequences of audio</li>
        <li>four concrete approaches to sampling from an LSTM with audio examples</li>
    </ul>
<!-- </div> -->



<!--=========================Audio files and picture=========================-->
<!-- <div class="containing_div"> -->
    <div class="highest_level" align="center">
        <div class="left_floating_div" id="input_audio_div">
            <br>
            <br>
            <p>Input Audio x</p>
            <audio controls>
              <source src="audio/input.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div id="rnn_image_div">
          <object id="overview" data="images/overview.svg" type="image/svg+xml"></object>
        </div>
        <div class="right_floating_div" id="output_audio_div">
            <br>
            <br>
            <p>Sampled Output &#x0177;</p>
            <audio controls>
              <source src="audio/column.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
    </div>
</div>

<!--=========================overview=========================-->
<hr>
<div class="containing_div" id='overview_cont'>
    <div class="half_size left_floating_div" id="lstm_image_div">
        <!-- <svg id="svg" shape-rendering="geometricPrecision">
        </svg> -->
        <object id="lstm_drawing" width='100%' data="images/lstm_drawing.svg" type="image/svg+xml"></object>
        <!-- <img id="lstm_image" src=lstm.png> -->
    </div>
    <div class="half_size right_floating_div" id="overview_text_div">
        <p class="centered_text">OVERVIEW</p>
        <p>Long Short Term Memory networks (LSTMs) are a type of Recurrent Neural Network (RNN).
           They were designed for modeling sequence data.
           An LSTM computes a prediction and a new cell state at each timestep.
           The process can be broken down into four conceptual steps.
           Hover over the equations for more information.
        </p>
            <br>
           <div id="equations_div">
               <math>
                  <span class="equation tooltip" id="f">     f<sub>t</sub> = &sigma;(W<sub>f</sub> &middot;[&ycirc;<sub>t-s</sub>, x<sub>t</sub>] + b<sub>f</sub>)</span><br>
                  <span class="equation tooltip" id="i">     i<sub>t</sub> = &sigma;(W<sub>i</sub> &middot;[&ycirc;<sub>t-s</sub>, x<sub>t</sub>] + b<sub>i</sub>)</span><br>
                  <span class="equation tooltip" id="c_hat"> &Ccirc;<sub>t</sub> = &sigma;(W<sub>C</sub> &middot;[&ycirc;<sub>t-s</sub>, x<sub>t</sub>] + b<sub>C</sub>)</span><br>
                  <span class="equation tooltip" id="c">     C<sub>t</sub> = f<sub>t</sub>*C<sub>t-1</sub>, i<sub>t</sub> * &Ccirc;<sub>t</sub></span><br>
                  <span class="equation tooltip" id="o">     o<sub>t</sub> = &sigma;(W<sub>o</sub> &middot;[&ycirc;<sub>t-s</sub>, x<sub>t</sub>] + b<sub>o</sub>)</span><br>
                  <span class="equation tooltip" id="y_hat"> &ycirc;<sub>t</sub> = o<sub>t</sub> * tanh(C<sub>t</sub>)</span><br>
               </math>
            </div>
    </div>
</div>


<!--=========================Training=========================-->
<hr>
<div class="containing_div" id="training_cont">
    <div class="left_floating_div half_size" id="training_text_div">
    <p class="centered_text"><span class="train_word">TRAINING</span></p>
    <p>LSTMs learn by making <span class="output_word">predictions</span> based on <span class="input_word">inputs</span>.
      The figure at right shows the process of <span class="train_word">training</span> a <span class="model_word">model</span>.
      At each time step the <span class="model_word">model</span> sees an array of <span class="input_word">inputs</span> and predicts an output for each array element.
      The accuracy of the prediction is evaluated by comparing it to a <span class="label_word">label</span> via the <span class="loss_word">loss</span> function.
      The error in the <span class="output_word">prediction</span>, the <span class="loss_word">loss</span>, is passed to the <span class="optim_word">optimizer</span>.
      The <span class="optim_word">optimizer</span> updates the internal weights in the <span class="model_word">model</span> via backpropagation.
      The result of this process is that the next time the <span class="model_word">model</span> sees the same <span class="input_word">input</span>, its <span class="output_word">prediction</span> will be more accurate.
    <pre>
for each time step:
    <span class='input_word'>input</span>, <span class="label_word">label</span>          = get_new_input_and_label()
    <span class='output_word'>prediction</span>, new_state = <span class="model_word">model</span>.predict(<span class="input_word">input</span>, prev_state)
    current_loss          = <span class="loss_word">loss</span>(<span class="label_word">label</span>, <span class="output_word">prediction</span>)
    <span class="optim_word">optimzer</span>.update_weights(current_loss)

    </pre>
    </div>
    <div class="right_floating_div half_size" id="training_image_div"><br>
      <!-- <img  src="training_futura.png"></div> -->
      <object id="training" width="100%" data="images/updated_training.svg" type="image/svg+xml"></object>
    </div>
</div>

<!--=========================Sampling =========================-->
<hr>
<div class="containing_div" id="sampling_cont">
    <div class="half_size left_floating_div" id="sampling_image_div">
        <!-- <img id="sampling_image" src="generation_futura.png"> -->
        <object id="sampling" width="100%" data="images/updated_sampling_drawing.svg" type="image/svg+xml"></object>
    </div>

    <div class="half_size right_floating_div" id="sampling_text_div"><br>
        <p class="centered_text"><span class="sampled_output_word">SAMPLING</span></p>
        <!-- <p>To sample an output the model sees inputs and makes predictions in the same manner as during training. The nature of of the input and the handling of the output is the main difference. The algorithm used for generating and output is depicted in the figure at left.  It can be summarized as the following: -->
        <p>To generate an <span class="sampled_output_word">sampled output</span> the <span class="model_word">model</span> is shown an arbitrary <span class="seed_word">seed</span>.
           It is asked what it thinks comes next in the sequence after the <span class="seed_word">seed</span>.
           Then the <span class='model_word'>model</span> is asked what it thinks comes after its first <span class="output_word">prediction</span> assuming it was correct.
           This process is repeated until the desired length is reached. </p>
        <p>To keep the seed the same length at each iteration the final <span class='keep_portion'>portion</span> of the
           <span class='output_word'>prediction</span> is appended to the end of the <span class="seed_word">seed</span>
           and the first portion of the <span class="seed_word">seed</span> is dequeued. </p>
        <pre>
<span class="seed_word">seed</span>   = get_new_seed()
<span class="sampled_output_word">output</span> = []
for each sampling time step:
    <span class="output_word">prediction</span>, new_state = <span class='model_word'>model</span>.predict(<span class="seed_word">seed</span>, prev_state)
    <span class="sampled_output_word">output</span>.append(<span class="keep_portion">prediction[-1]</span>)
    <span class="seed_word">seed</span>.append(<span class="keep_portion">prediction[-1]</span>
    <span class="seed_word">seed</span> = <span class="seed_word">seed</span>[1:]

        </pre>
    </div>
</div>

<!--=========================sampling methods and psuedocode=========================-->
<hr>
<div class="containing_div" id="methods_cont">
    <div class="row">
        <div class="code_div" id="sequential_pseudocode_div">
        <p class="centered_text">SEQUENTIAL</p>
        <pre>
for each epoch:
    for each timestep:
        input        = get_input()
        label        = get_label()

        y_hat, state = model.guess(input, state)
        loss         = loss(y_hat, label)
        model.update_weights(loss)
        <span class="pause_training_word">if timestep % wait == 0:</span>
            seed   = get_seed()
            output = []
            for each dream_timestep:
                pred = model.guess(seed)
                output.append(pred[-1])
                seed.append(pred[-1])
                seed = seed[1:]




        </pre>
        </div>
        <div id="sampling_methods_text_div">
            <p>When sampling from a model the simplest approach would be to <span class="pause_training_word">periodically pause training</span> to sample an output.
              This algorithm is shown in psuedocode to the left. In practice this method works some of the time but can get stuck predicting the same output over and over.
              The pseudocode at right shows a slighlty different algorithm that performs better in practice.
              This method has the model <span class="concurrent_word">train and sample concurrently</span>.
        </div>
        <div class="code_div" id="concurrent_pseudocode_div">
        <p class="centered_text">CONCURRENT</p>
        <pre>
seed   = get_seed()
output = []
for each epoch:
    for each timestep:
        <span class="concurrent_word">#train normally</span>
        input= get_input()
        label = get_label()
        y_hat, state = model.guess(input, state)
        loss  = loss(y_hat, label)
        model.update_weights(loss)

        <span class="concurrent_word">#make a prediction</span>
        pred = model.guess(seed)
        output.append(pred[-1])
        seed.append(pred[-1])
        seed = seed[1:]
        if len(output) == output_length:
            save_output()
            output = []
            seed   = get_seed()
        </pre>
        </div>
    </div>
</div>

<!--=========================Vector Approach=========================-->
<hr>
<div class="containing_div" id="vector_cont">
    <div class="half_size left_floating_div" id="vector_text_div"><br><br>
    <p class="centered_text">VECTOR APPROACH</p>
    <p>The first approach to sampling from an LSTM has the model predict a vector of audio samples for each input vector.
       These vectors are the <span class="vector_len_word">same size</span> and can contain overlapping or non-overlapping samples.
       This approach is the fastest of the four presented here.
       The predicted vectors can sometimes have discontinuities at their extremities which are problematic.
    </div>
    <div class="half_size right_floating_div" id="vector_image_div">
        <!-- <img src="vector_image.png" width="80%" id="vector_image"> -->
        <object id="vector_data" width="100%" data="images/vector_data.svg" type="image/svg+xml"></object>


    </div>
</div>


<!--=========================Vector Approach=========================-->
<hr>
<div class="containing_div" id="vector_audio_cont">
    <div class="half_size left_floating_div">
        <p class="centered_text">VECTOR APPROACH AUDIO</p>
        <div class="audio_div">
            <p>Vector length 1024</p>
            <audio controls>
              <source src="audio/vector_1024.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div class="audio_div">
            <p>Vector length 512</p>
            <audio controls>
              <source src="audio/vector_512.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div class="audio_div">
            <p>Vector length 256</p>
            <audio controls>
              <source src="audio/vector_256.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div class="audio_div">
            <p>Vector length 128</p>
            <audio controls>
              <source src="audio/vector_128.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
    </div>
    <div class="half_size right_floating_div">
        <br><br><br><br><br>
        <p class="centered_text">VECTOR APPROACH ANALYSIS</p>
        <p>Changing the length of the predicted vectors drastically changes the quality of the sampled outputs.
           When the vectors are long, the model produces samples that evolve slowly.
           When the vectors are short, the model produces samples that change more rapidly but resemble in the input data less.
    </div>
</div>
<!--=========================Magnitude Spectrum Approach=========================-->
<br>
<hr>
<div class="containing_div" id="magnitude_cont">
    <div class="half_size left_floating_div"><br><br>
        <p class="centered_text">MAGNITUDE SPECTRUM APPROACH</p>
        <p>This approach formats the data in the same manner as the vector approach except that each vector is <span class="magnitude_word">magnitude spectrum window</span>.
           The raw audio is broken into vectors as before, then each vector is transformed via the <a href="https://ccrma.stanford.edu/~jos/parshl/Outline_Program.html">FFT</a>.
           The phase information is omitted and the magnitudes are normalized to the range 0 to 1.
           The sampled outputs are resynthesized via a channel vocoder.
           It works by linearly interpolating between the transpose of the predicted windows.
           This produces a <span class="interpolated_word">sampling rate envelope</span> for each <span class="bin_word">bin</span> of the FFT.
           Each envelope is applied to an <span class="oscillator_word">oscillator</span> with its frequency set to the center frequency of the corresponding <span class="bin_word">FFT bin</span>.
    </div>

    <div class="half_size right_floating_div" id="magnitude_spectrum_image_div">
        <br><br>
        <!-- <img src="vector_image.png" id="magnitude_spectrum_image"> -->
        <object id="magnitude_data" width="100%" data="images/vector_data.svg" type="image/svg+xml"></object>

    </div>
</div>
<br>
<br>

<hr>
<div class="containing_div" id="vocoder_cont">
    <br>
    <p class="centered_text">VOCODER DESIGN</p>
    <div id="vocoder_image_div">
      <object id="vocoder" width="90%" data="images/vocoder.svg" type="image/svg+xml"></object>
    </div>
</div>
<hr>
<div class="containing_div" id="mag_audio_cont">
    <div class="half_size left_floating_div">
        <p class="centered_text">MAGNITUDE SPECTRUM APPROACH AUDIO</p>
        <div class="audio_div">
            <p>Hop Size 1024</p>
            <audio controls>
              <source src="audio/fft_1024.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div class="audio_div">
            <p>Hop Size 512</p>
            <audio controls>
              <source src="audio/fft_512.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div class="audio_div">
            <p>Hop Size 256</p>
            <audio controls>
              <source src="audio/fft_256.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
        <div class="audio_div">
            <p>Hop Size 128</p>
            <audio controls>
              <source src="audio/fft_128.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>

        <div class="audio_div">
            <p>Hop Size 56</p>
            <audio controls>
              <source src="audio/fft_56.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
    </div>
    <div class="half_size right_floating_div ">
        <br><br><br>
        <p class="centered_text">MAGNITUDE SPECTRUM ANALYSIS</p>
        <p>As with the vector approach vector length, changing the <a href="https://ccrma.stanford.edu/~jos/parshl/Choice_Hop_Size.html">hop size</a> of the fft used to produce the data drastically changes the quality of the sampled outputs.
           When the hop size is large the frequencies are overlapped and blurred to create a subtle evolving texture.
           As the hop size is decreased transitions between different pitches become increasingly clear.
           When the hop size is very small sequences of notes from the input data can be observered, however the model also tends to become somewhat unstable.
    </div>
</div>

<!--=========================Transpose Approach=========================-->
<hr>
<div class="containing_div" id="transpose_cont">
    <div class="half_size left_floating_div">
        <p class="centered_text">TRANSPOSE APPROACH</p>
        <p>This approach features the most exotic handling of the model outputs.
           The output of the LSTM is the same shape as the input <code><span class='trans_size_word'>[number_of_unrollings, vector_length]</span></code>.
           It is passed through a fully connected layer to change the shape to <code>[number_of_unrollings, 1]</code>.
           The transpose of this column vector is passed through another fully connected layer to change the shape to <code><span class="one_by_one_word">[1, 1]</span></code>.
        </p>
    </div>
    <div class="half_size right_floating_div">
        <!-- <img src="transpose.png" id="transpose_image"> -->
        <object id="transpose_data" width="100%" data="images/transpose_data.svg" type="image/svg+xml"></object>

    </div>
</div>

<hr>
<div class="containing_div" id="trans_audio_cont">
    <div class="half_size left_floating_div">
        <p class="centered_text">TRANSPOSE APPROACH AUDIO</p>
        <div class="audio_div">
            <p>Transpose</p>
            <audio controls>
              <source src="audio/transpose.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
    </div>
    <div class="half_size right_floating_div">
        <p class="centered_text">TRANSPOSE ANALYSIS</p>
        <p>This method hardly ever produces noisy outputs. This method is the most successful in that aspect.
           The outputs themselves tend to blend all the frequences in the seed.
           The biggest drawback of this approach is that it takes significantly more time to produce an output compared to the previous two approaches.
           This is due to the fact the the model is predicting one audio sample at a time as opposed to a vector of samples at a time.
    </div>
</div>

<!--=========================Column Vector Approach=========================-->
<hr>
<div class="containing_div" id="column_cont">
    <div class="half_size left_floating_div">
        <p class="centered_text">COLUMN VECTOR APPROACH</p>
        <p>This the most successful approach. The model predicts one sample for each input vector.
           This is accomplished by passing the output of the LSTM through a fully connected layer.
           The sampled outputs from this model tend to feature notes from the input data, but rarely in the same order.
           Often these notes are predicted in a different order than they appear in the input data.
           Occasionally the notes are surreally distorted.
           For instance piano notes are sustained or growing in volume rather than decaying after their initial attack.
           Because of the way the input data is offset from the label data only the final element in the predicted vector is not present elsewhere in the seed.
           This final audio sample is the only piece that is kept during sampling. As such this approach is very slow to produce outputs like the tranpose approach.
        </p>
    </div>
        <div class="half_size right_floating_div">
            <!-- <img src="column_vector.png" id="column_vector_image"> -->
            <object id="column_data" width="100%" data="images/column_data.svg" type="image/svg+xml"></object>

        </div>
</div>

<hr>
<div class="containing_div" id="column_audio_cont">
    <div class="half_size left_floating_div">
        <p class="centered_text">COLUMN VECTOR APPROACH AUDIO</p>
        <div class="audio_div">
            <p>Column Vector</p>
            <audio controls>
              <source src="audio/column.mp3" type="audio/mp3">
            Your browser does not support the audio element.
            </audio>
        </div>
    </div>

    <div class="half_size right_floating_div">
        <p class="centered_text">COLUMN VECTOR ANALYSIS</p>
        <p>The sampled outputs produced with this approach resemble most closely what a human might guess if presented with the same task as the LSTM.
           Notes from the input are repeated, but in a different order. Some minor changes might be made to the data, but generally the data remains recognizable.
           This is in stark contrast to the other methods that produce outputs where sometimes the data is only barely discernible.
    </div>
</div>
<script>

var yellow   = '#faf051';
var orange   = '#f5aa27';
var d_orange = '#fb7147';
var sienna   = '#c78720';
var red      = '#fb050a';
var maroon   = '#be0400';
var purple   = '#962a58';
var l_green  = '#b4c524';
var green    = '#35b73e';
var d_green  = '#046437';
var teal_1   = '#35c59f';
var teal_2   = '#0fafa8';
var teal_3   = '#71c3c4';
var blue     = '#003e7d';
var d_blue   = '#052239';
var d_grey   = '#3a444c';
var l_grey   = '#a6b6b5';

function add_animations(){

    var drawing        = document.getElementById('lstm_drawing');
    var content        = drawing.contentDocument;

    var forget_gate    = content.getElementById("forget_gate");
    var input_gate     = content.getElementById("input_gate");
    var update_gate    = content.getElementById("update_gate");
    var output_gate    = content.getElementById("output_gate");
    var forget_sigmoid = content.getElementById("forget_sigmoid");
    var input_sigmoid  = content.getElementById("input_sigmoid");
    var input_tanh     = content.getElementById("input_tanh");
    var input_mult     = content.getElementById("input_mult");
    var update_mult    = content.getElementById("update_mult");
    var update_add     = content.getElementById("update_add");
    var output_sigmoid = content.getElementById("output_sigmoid");
    var output_tanh    = content.getElementById("output_tanh");
    var output_mult    = content.getElementById("output_mult");
    //equations
    var f              = document.getElementById('f');
    var i              = document.getElementById('i');
    var c_hat          = document.getElementById('c_hat');
    var c              = document.getElementById('c');
    var o              = document.getElementById('o');
    var y_hat          = document.getElementById('y_hat');




    //groups of words
    var seed_words      = document.getElementsByClassName('seed_word');
    var output_words    = document.getElementsByClassName('output_word');
    var keep_only       = document.getElementsByClassName('keep_portion');
    var model_words     = document.getElementsByClassName('model_word');
    var sampled_output  = document.getElementsByClassName('sampled_output_word');
    var loss_words      = document.getElementsByClassName('loss_word');
    var input_words     = document.getElementsByClassName('input_word');
    var label_words     = document.getElementsByClassName('label_word');
    var optim_words     = document.getElementsByClassName('optim_word');
    var train_words     = document.getElementsByClassName('train_word');
    var pause_words     = document.getElementsByClassName('pause_training_word');
    var concurrent      = document.getElementsByClassName('concurrent_word');
    var vector_len_words= document.getElementsByClassName('vector_len_word');
    var magnitude_words = document.getElementsByClassName('magnitude_word');
    var osc_words       = document.getElementsByClassName('oscillator_word');
    var interp_words    = document.getElementsByClassName('interpolated_word');
    var bin_words       = document.getElementsByClassName('bin_word');
    var one_one_words   = document.getElementsByClassName('one_by_one_word');
    var trans_words     = document.getElementsByClassName('trans_size_word');



    //svgs
    var sampling_svg    = document.getElementById('sampling').contentDocument;
    var training_svg    = document.getElementById('training').contentDocument;
    var vector_svg      = document.getElementById('vector_data').contentDocument;
    var magnitude_svg   = document.getElementById('magnitude_data').contentDocument;
    var vocoder_svg     = document.getElementById('vocoder').contentDocument;
    var transpose_svg   = document.getElementById('transpose_data').contentDocument;

    //boxes in svgs
    var sampling_seed   = sampling_svg.getElementById('sampling_seed_box');
    var sampling_output = sampling_svg.getElementById('sampling_output_box');
    var keep_0          = sampling_svg.getElementById('sampling_keep_text_0');
    var keep_1          = sampling_svg.getElementById('sampling_keep_portion_1');
    var sampling_lstm   = sampling_svg.getElementById('sampling_lstm_box');

    var training_loss   = training_svg.getElementById('loss');
    var training_input  = training_svg.getElementById('input');
    var training_label  = training_svg.getElementById('training_label_box');
    var training_output = training_svg.getElementById('training_output_box');
    var training_optim  = training_svg.getElementById('optimizer');

    var vector_len_0    = vector_svg.getElementById('vector_length_box_0');
    var vector_len_1    = vector_svg.getElementById('vector_length_box_1');

    var magnitude_box_0 = magnitude_svg.getElementById('vector_box_0');
    var magnitude_box_1 = magnitude_svg.getElementById('vector_box_1');

    var bin_box_0       = vocoder_svg.getElementById('bins_box_0');
    var bin_box_1       = vocoder_svg.getElementById('bins_box_1');

    var interp_box      = vocoder_svg.getElementById('interpolated');

    var osc_0           = vocoder_svg.getElementById('osc_0');
    var osc_1           = vocoder_svg.getElementById('osc_1');
    var osc_2           = vocoder_svg.getElementById('osc_2');
    var osc_3           = vocoder_svg.getElementById('osc_3');
    var osc_4           = vocoder_svg.getElementById('osc_4');

    var trans_unrolling = transpose_svg.getElementById('unrollings_box');
    var trans_vec_len   = transpose_svg.getElementById('vector_length_box');
    var one_0           = transpose_svg.getElementById('one_0');
    var one_1           = transpose_svg.getElementById('one_1');




    over(f,     forget_gate, red,       'text_fade_red  3s',    'text_fade_red');
    over(i,     input_gate,  green,     'text_fade_green   3s', 'text_fade_green');
    over(c_hat, input_gate,  l_green,   'text_fade_l_green 3s', 'text_fade_l_green');
    over(c,     update_gate, purple,    'text_fade_purple  3s', 'text_fade_purple');
    over(o,     output_gate, orange,    'text_fade_orange  3s', 'text_fade_orange');
    over(y_hat, output_gate, yellow,    'text_fade_yellow  3s', 'text_fade_yellow');

    highlight_group(seed_words,     [sampling_seed],                      green,    'text_fade_green   3s');
    highlight_group(output_words,   [sampling_output],                    teal_2,   'text_fade_teal_2  3s');
    highlight_group(keep_only,      [keep_1],                             sienna,   'text_fade_sienna  3s');
    highlight_group(model_words,    [sampling_lstm],                      l_green,  'text_fade_l_green 3s');
    highlight_group(sampled_output, [],                                   purple,   'text_fade_purple  3s');
    highlight_group(train_words,    [],                                   d_green,  'text_fade_d_green 3s');

    highlight_group(loss_words,     [training_loss],                      orange,      'text_fade_orange    3s');
    highlight_group(input_words,    [training_input],                     d_orange,   'text_fade_d_orange 3s');
    highlight_group(label_words,    [training_label],                     teal_1,   'text_fade_yellow 3s');
    highlight_group(optim_words,    [training_optim],                     yellow,    'text_fade_yellow  3s');


    highlight_group(pause_words,    [],                                   maroon,   'text_fade_maroon  3s');
    highlight_group(concurrent,     [],                                   sienna,    'text_fade_sienna   3s');

    highlight_group(vector_len_words, [vector_len_0,    vector_len_1],    d_green,   'text_fade_d_green 3s');
    highlight_group(magnitude_words,  [magnitude_box_0 , magnitude_box_1],purple,   'text_fade_purple 3s');

    highlight_group(bin_words,    [bin_box_0 , bin_box_1],                purple,  'text_fade_purple 3s');
    highlight_group(interp_words, [interp_box],                           maroon,  'text_fade_maroon 3s');
    highlight_group(osc_words,    [osc_0, osc_1, osc_2, osc_3, osc_4],    teal_3,  'text_fade_teal_3 3s');

    highlight_group(one_one_words,[one_0, one_1],                         l_green,  'text_fade_l_green 3s');
    highlight_group(trans_words,  [trans_unrolling, trans_vec_len],       orange,  'text_fade_orange 3s');

    handle_containing_divs();
    add_tooltips();
}


function highlight_group(group, children, color, animation){

    for (var p=0; p<group.length; p++){
        var cur_item = group[p];
        cur_item.onmouseover = function(){
                                  for(var i=0;i<group.length; i++){
                                      group[i].style.background  = color;
                                  }
                                  for(var k=0; k<children.length; k++){
                                      if (children[k] != null){
                                          children[k].style['fill']         = color;
                                          children[k].style['fill-opacity'] = '0.75';
                                      }
                                  }
                              };
        cur_item.onmouseout  = function(){
                                  for(var i=0;i<group.length; i++){
                                      var current_1             = group[i];
                                      current_1.style.animation = animation;
                                      current_1.style['-webkit-animation-name'] = animation;
                                      current_1.addEventListener('animationend', function(){
                                                                                    this.style.animation='';
                                                                                    this.style.background = 'rgba(0,0,0,0)';
                                                                                 });
                                  }
                                  for(var l=0; l<children.length; l++){

                                      if (children[l] != null){
                                          children[l].style.animation = 'fade 3s';
                                          children[l].style['-webkit-animation-name'] = 'fade 3s';
                                          children[l].addEventListener('animationend', function(){
                                                                                            this.style.animation='';
                                                                                            this.style['-webkit-animation-name']='';
                                                                                            this.style['fill-opacity'] = '0.0';
                                                                                       });
                                      }
                                  }
                              } ;
    }

}

function over(parent, child, color, text_animation, safari_anim){
    //parent is always an equation child is always a gate
    parent.onmouseover = function(){
                              parent.style.background     = color;
                              child.style['fill-opacity'] = '1';
                              child.style.fill            = color;
                         };

    parent.onmouseout  = function(){
                              parent.style.animation                     = text_animation;
                              parent.style['-webkit-animation-name']     = safari_anim;
                              parent.style['-webkit-animation-duration'] = '3s';
                              parent.addEventListener('animationend', function(){this.style.animation=''; this.style['-webkit-animation-name'] = ''; this.style.background     = 'rgba(0, 0, 0, 0)';});
                              child.style.animation = 'fade 3s';
                              child.style['-webkit-animation-name']      = 'fade';
                              child.style['-webkit-animation-duration']  = '3s';

                              child.addEventListener('animationend',  function(){this.style.animation=''; this.style['-webkit-animation-name'] = ''; this.style['fill-opacity'] = '0';});
                         };

    child.onmouseover  = function(){
                              child.style.fill            = color;
                              child.style['fill-opacity'] = '0.75';
                              parent.style.background     = color;

                         }
    child.onmouseout   = function(){
                              parent.addEventListener('animationend', function(){this.style.animation='';   this.style['-webkit-animation-name'] = '';  this.style.background     = 'rgba(0, 0, 0, 0)';});
                              child.addEventListener('animationend',  function(){this.style.animation='';   this.style['-webkit-animation-name'] = '';  this.style['fill-opacity'] = '0.0';});

                              child.style.animation                    = 'fade 3s';
                              child.style['-webit-animation-name']     = 'fade';
                              child.style['-webit-animation-duration'] = '3s';


                              parent.style.animation                     = text_animation;
                              parent.style['-webkit-animation-name']     = safari_anim;
                              parent.style['-webkit-animation-duration'] = '3s';
                         }

}

function handle_containing_divs(){




    animate('title_cont',        l_green,   'text_fade_l_green 3s', 'text_fade_l_green');
    animate('overview_cont',     teal_2,    'text_fade_teal_2  3s', 'text_fade_teal_2');
    animate('training_cont',     teal_1,    'text_fade_teal_1  3s', 'text_fade_teal_1');
    animate('sampling_cont',     orange,    'text_fade_orange  3s', 'text_fade_orange');
    animate('methods_cont',      l_green,   'text_fade_l_green 3s', 'text_fade_l_green');
    animate('vector_cont',       yellow,    'text_fade_yellow  3s', 'text_fade_yellow');
    animate('vector_audio_cont', teal_1,    'text_fade_teal_1  3s', 'text_fade_teal_1');
    animate('magnitude_cont',    orange,    'text_fade_orange  3s', 'text_fade_orange');
    animate('vocoder_cont',      l_green,   'text_fade_l_green 3s', 'text_fade_l_green');
    animate('mag_audio_cont',    yellow,    'text_fade_yellow  3s', 'text_fade_yellow');
    animate('transpose_cont',    red,       'text_fade_red     3s', 'text_fade_red');
    animate('trans_audio_cont',  purple,    'text_fade_purple  3s', 'text_fade_purple');
    animate('column_cont',       l_green,   'text_fade_l_green 3s', 'text_fade_l_green');
    animate('column_audio_cont', purple,    'text_fade_purple  3s', 'text_fade_purple');


}

function animate(item_name, color, animation){
    var item = document.getElementById(item_name);
    item.onmouseover =     function(){this.style.background = color;};
    item.onmouseout  =     function(){this.style.background  = 'rgba(0,0,0,0)';};
}

function make_tooltip(eq, msg,  id, offset){
    var tooltip         = document.createElement('span');
    tooltip.innerHTML   = msg;
    tooltip.id          = id;
    tooltip.style.top   = offset;
    tooltip.classList.add('tooltiptext');
    eq.appendChild(tooltip);
}

function add_tooltips(){
    var msgs = ['The forget gate decides what should be erased from the cell state at the next timestep.',
                'The input gate decides which elements from the current input should be added to the cell state.',
                'The input gate also computes a vector of candidate values that will be selectively added to the cell state.',
                'The update step applies the decisions made by the forget and input gates to produce the new cell state.',
                'The output gate decides based on the input, which elements from the cell state will be output.',
                'The output gate output for each timestep is computed based on the current input and the updated cell state. '];
    var items = [f, i, c_hat, c, o, y_hat];
    var ids   = ['f_tooltip', 'i_tooltip', 'c_hat_tooltip', 'c_tooltip', 'o_tooltip', 'y_hat_tooltip'];
    var offs  = ['-30px', '-40px', '-50px', '-60px', '-70px', '-80px'];

    for (var item=0; item< msgs.length; item++){
        make_tooltip(items[item], msgs[item], ids[item], offs[item]);
    }



}

</script>
</body>
</html>
